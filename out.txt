import os
import hashlib
import logging
from datetime import datetime, timedelta
import asyncio
import aiofiles
from concurrent.futures import ThreadPoolExecutor
import sqlite3

# Constants and configurations
READ_CHUNK_SIZE = 1048576  # Read chunk size for network files (default: 1MB)
MAX_WORKERS = 20  # Maximum number of worker threads
PROGRESS_INTERVAL = 10  # Interval in seconds for logging progress

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def create_database(db_file):
    conn = sqlite3.connect(db_file)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS file_hashes (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT)''')
    conn.commit()
    return conn

def insert_file_hash(conn, file_path, file_hash):
    c = conn.cursor()
    c.execute("INSERT OR REPLACE INTO file_hashes (file_path, file_hash) VALUES (?, ?)", (file_path, file_hash))
    conn.commit()

def load_checkpoint(conn):
    c = conn.cursor()
    c.execute("SELECT file_path FROM file_hashes")
    rows = c.fetchall()
    return {row[0] for row in rows}

def calculate_remaining_time(elapsed_time, processed_count, total_count):
    if processed_count == 0:
        return timedelta(seconds=0)
    average_time_per_file = elapsed_time / processed_count
    remaining_files = total_count - processed_count
    remaining_time = average_time_per_file * remaining_files
    return remaining_time

async def hash_file(file_path, chunk_size=READ_CHUNK_SIZE):
    """Hash file using MD5 algorithm."""
    if os.name == 'nt' and len(file_path) > 260:
        file_path = f'\\\\?\\{file_path}'

    md5_hash = hashlib.md5()
    try:
        async with aiofiles.open(file_path, "rb") as f:
            while True:
                byte_block = await f.read(chunk_size)
                if not byte_block:
                    break
                md5_hash.update(byte_block)
        return file_path, md5_hash.hexdigest()
    except Exception as e:
        logging.error("Error hashing file %s: %s", file_path, e)
        return file_path, None

async def process_files_batch(file_paths, conn, chunk_size=READ_CHUNK_SIZE):
    """Process batch of files and return their hashes."""
    tasks = [hash_file(file_path, chunk_size) for file_path in file_paths]
    results = await asyncio.gather(*tasks)
    for result in results:
        if result and result[1]:  # Ensure that both file_path and file_hash are not None
            insert_file_hash(conn, result[0], result[1])

async def hash_files_in_folder(folder, conn, chunk_size=READ_CHUNK_SIZE):
    """Hash files in specified folder asynchronously."""
    start_time = datetime.now()
    file_paths = []

    # Collect file paths in the folder
    for root, dirs, files in os.walk(folder):
        for file in files:
            file_path = os.path.join(root, file)
            file_paths.append(file_path)

    # Load checkpoint to skip already processed files
    processed_files = load_checkpoint(conn)
    file_paths = [fp for fp in file_paths if fp not in processed_files]

    total_files = len(file_paths)
    processed_count = 0

    # Process files in batches asynchronously
    for i in range(0, total_files, MAX_WORKERS):
        await process_files_batch(file_paths[i:i + MAX_WORKERS], conn, chunk_size)
        processed_count += len(file_paths[i:i + MAX_WORKERS])

        # Calculate and log progress
        elapsed_time = datetime.now() - start_time
        remaining_time = calculate_remaining_time(elapsed_time, processed_count, total_files)
        logging.info(f"Processed {processed_count}/{total_files}. Elapsed time: {elapsed_time}. Estimated remaining time: {remaining_time}")

    # Log final elapsed time
    elapsed_time = datetime.now() - start_time
    logging.info(f"Total Elapsed Time: {elapsed_time}")

def main():
    """Main function to orchestrate file hashing process."""
    logging.info("Program started.")
    start_time = datetime.now()

    # Prompt user for input
    source_folder = input("Enter the source folder containing the files to hash: ")
    output_db = input("Enter the full path for the output database file: ")

    # Create database and hash files in the folder asynchronously
    conn = create_database(output_db)
    asyncio.run(hash_files_in_folder(source_folder, conn))
    conn.close()

    # Log processing time
    end_time = datetime.now()
    processing_time = end_time - start_time
    logging.info("Program finished. Processing time: %s", processing_time)

if __name__ == "__main__":
    main()
