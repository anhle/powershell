import os
import hashlib
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import psutil
import sqlite3

BUFFER_SIZE_THRESHOLD = 10000  # Threshold for buffered writing to CSV
CHECKPOINT_INTERVAL = 1000  # Interval for saving checkpoint data

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

def get_folder_path(prompt):
    while True:
        folder_path = input(prompt)
        if os.path.isdir(folder_path):
            return folder_path
        else:
            print("The folder does not exist. Please try again.")
            logging.error("The folder does not exist: %s", folder_path)

def get_output_path(prompt):
    while True:
        output_path = input(prompt)
        if os.path.isdir(os.path.dirname(output_path)):
            return output_path
        else:
            print("The output folder does not exist. Please try again.")
            logging.error("The output folder does not exist: %s", os.path.dirname(output_path))

def create_database(db_file):
    conn = sqlite3.connect(db_file)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS file_hashes (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT)''')
    conn.commit()
    return conn

def insert_file_hash(conn, file_path, file_hash):
    c = conn.cursor()
    c.execute("INSERT OR REPLACE INTO file_hashes (file_path, file_hash) VALUES (?, ?)", (file_path, file_hash))
    conn.commit()

def load_checkpoint(conn):
    c = conn.cursor()
    c.execute("SELECT file_path FROM file_hashes")
    rows = c.fetchall()
    checkpoint = {row[0] for row in rows}
    return checkpoint

def hash_file(file_path):
    if os.name == 'nt':  # Check if the OS is Windows
        if len(file_path) > 260:
            file_path = f'\\\\?\\{file_path}'
    
    md5_hash = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(65536), b""):  # Read in larger chunks
                md5_hash.update(byte_block)
        return file_path, md5_hash.hexdigest()
    except Exception as e:
        logging.error("Error hashing file %s: %s", file_path, e)
        return file_path, None

def hash_files_in_folder(folder, conn):
    buffer = []
    start_time = datetime.now()
    processed_count = 0
    checkpoint = load_checkpoint(conn)

    for root, dirs, files in os.walk(folder):
        for file in files:
            file_path = os.path.join(root, file)
            if file_path not in checkpoint:
                file_hash = hash_file(file_path)
                if file_hash:
                    buffer.append(file_hash)
                    if len(buffer) >= BUFFER_SIZE_THRESHOLD:
                        for file_hash in buffer:
                            insert_file_hash(conn, *file_hash)
                        buffer.clear()

                    processed_count += 1
                    if processed_count % CHECKPOINT_INTERVAL == 0:
                        for file_hash in buffer:
                            insert_file_hash(conn, *file_hash)
                        buffer.clear()

                    # Calculate elapsed time and estimated remaining time
                    elapsed_time = datetime.now() - start_time
                    total_files = sum([len(files) for r, d, files in os.walk(folder)])
                    files_left = total_files - processed_count
                    time_per_file = elapsed_time / processed_count if processed_count > 0 else 0
                    estimated_remaining_time = time_per_file * files_left
                    logging.info(f"Elapsed Time: {elapsed_time}, Estimated Remaining Time: {estimated_remaining_time}")

    # Write remaining data in buffer to database
    if buffer:
        for file_hash in buffer:
            insert_file_hash(conn, *file_hash)
        buffer.clear()

    return True

def export_to_csv(db_file, csv_file):
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()
    cursor.execute("SELECT file_path, file_hash FROM file_hashes")
    
    with open(csv_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['file_path', 'file_hash'])  # Write the header
        for row in cursor.fetchall():
            writer.writerow(row)
    
    conn.close()
    logging.info(f"Database contents have been exported to {csv_file}")

def monitor_resources(interval=1):
    try:
        while True:
            cpu_usage = psutil.cpu_percent(interval=interval)
            memory_info = psutil.virtual_memory()
            memory_usage = memory_info.percent
            thread_count = threading.active_count()
            logging.info(f"CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%, Active Threads: {thread_count}")
            print(f"CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%, Active Threads: {thread_count}")
    except KeyboardInterrupt:
        print("Monitoring stopped.")

def main():
    logging.info("Program started.")
    start_time = datetime.now()

    source_folder = get_folder_path("Enter the source folder containing the files to hash: ")
    output_db = get_output_path("Enter the full path for the output database file: ")
    output_csv = get_output_path("Enter the full path for the output CSV file: ")

    conn = create_database(output_db)

    # Start resource monitoring in a separate thread
    monitor_thread = threading.Thread(target=monitor_resources)
    monitor_thread.daemon = True
    monitor_thread.start()

    hash_files_in_folder(source_folder, conn)

    conn.close()

    # Export database to CSV
    export_to_csv(output_db, output_csv)

    end_time = datetime.now()
    processing_time = end_time - start_time
    logging.info("Program finished. Processing time: %s", processing_time)
    print(f"Hashes have been saved to {output_db} and exported to {output_csv}")

if __name__ == "__main__":
    main()
