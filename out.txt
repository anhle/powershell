import os
import hashlib
import csv
import logging
import logging.handlers
import concurrent.futures
import sys
import psutil
import time
import json

CHUNK_SIZE = 1024 * 1024  # Read files in chunks of 1 MB
MEMORY_LIMIT = 1 * 1024 * 1024 * 1024  # 1GB memory limit for batching
MAX_WORKERS = 40  # Adjusted for better utilization
MONITOR_INTERVAL = 1  # Interval for monitoring resource usage in seconds
PROCESSED_FILES_RECORD = 'processed_files.json'  # File to store processed files record
LOG_FILE = 'file_hashing.log'  # Log file name
LOG_SIZE_LIMIT = 10 * 1024 * 1024  # Log file size limit of 10 MB
BACKUP_COUNT = 5  # Keep up to 5 backup log files

def get_folder_path(prompt):
    folder_path = input(prompt)
    while not os.path.isdir(folder_path):
        print("The specified folder does not exist. Please try again.")
        folder_path = input(prompt)
    return folder_path

def get_file_path(prompt):
    file_path = input(prompt)
    return file_path

def hash_file(file_path):
    try:
        hasher = hashlib.sha256()
        with open(file_path, 'rb') as file:
            while chunk := file.read(CHUNK_SIZE):
                hasher.update(chunk)
        return file_path, hasher.hexdigest(), None
    except Exception as e:
        return file_path, None, str(e)

def write_to_csv(file_hashes, output_file):
    with open(output_file, mode='a', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(file_hashes)

def monitor_resources():
    process = psutil.Process()
    while True:
        print(f"CPU Usage: {psutil.cpu_percent(interval=1)}%")
        print(f"Memory Usage: {psutil.virtual_memory().percent}%")
        print(f"Network Usage: {psutil.net_io_counters().bytes_sent / (1024*1024):.2f} MB sent, {psutil.net_io_counters().bytes_recv / (1024*1024):.2f} MB received")
        print(f"Active Threads: {len(process.threads())}")
        time.sleep(MONITOR_INTERVAL)

def load_processed_files(record_file):
    if os.path.exists(record_file):
        with open(record_file, 'r') as file:
            return set(json.load(file))
    return set()

def save_processed_files(processed_files, record_file):
    with open(record_file, 'w') as file:
        json.dump(list(processed_files), file)

def normalize_path(file_path):
    # On Windows, use the \\?\ prefix for long paths
    if os.name == 'nt' and len(file_path) > 260:
        return f"\\\\?\\{file_path}"
    return file_path

def setup_logging():
    logger = logging.getLogger()
    logger.setLevel(logging.ERROR)
    
    handler = logging.handlers.RotatingFileHandler(
        LOG_FILE, maxBytes=LOG_SIZE_LIMIT, backupCount=BACKUP_COUNT
    )
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    
    logger.addHandler(handler)
    return logger

def get_relative_file_paths(folder):
    file_paths = []
    for root, _, files in os.walk(folder):
        for file in files:
            full_path = os.path.join(root, file)
            relative_path = os.path.relpath(full_path, folder)
            file_paths.append((relative_path, full_path))
    return file_paths

def compare_hashes(source_hashes, destination_hashes):
    discrepancies = []
    for rel_path, source_hash in source_hashes.items():
        dest_hash = destination_hashes.get(rel_path)
        if dest_hash is None:
            discrepancies.append((rel_path, "MISSING IN DESTINATION", source_hash, ""))
        elif source_hash != dest_hash:
            discrepancies.append((rel_path, "DIFFERENT HASH", source_hash, dest_hash))
    for rel_path in destination_hashes.keys():
        if rel_path not in source_hashes:
            discrepancies.append((rel_path, "MISSING IN SOURCE", "", destination_hashes[rel_path]))
    return discrepancies

def main():
    # Configure logging
    logger = setup_logging()

    # Get the folders containing the files to hash and compare
    source_folder = get_folder_path("Please enter the path to the source folder: ")
    destination_folder = get_folder_path("Please enter the path to the destination folder: ")

    # Get the location to save the hash values
    output_file = get_file_path("Please enter the path to the CSV file to save the hash values: ")

    # Load the record of processed files
    processed_files = load_processed_files(PROCESSED_FILES_RECORD)

    # Create or clear the CSV file if starting fresh
    if not processed_files:
        with open(output_file, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["File Path", "SHA-256 Hash"])

    # Collecting file paths
    source_files = get_relative_file_paths(source_folder)
    destination_files = get_relative_file_paths(destination_folder)

    source_file_paths = [normalize_path(path[1]) for path in source_files]
    destination_file_paths = [normalize_path(path[1]) for path in destination_files]

    # Start resource monitoring in a separate thread
    monitor_thread = concurrent.futures.ThreadPoolExecutor(max_workers=1).submit(monitor_resources)

    # Hash files in parallel using multiprocessing
    source_hashes = {}
    destination_hashes = {}

    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {executor.submit(hash_file, file_path): file_path for file_path in source_file_paths}
        for future in concurrent.futures.as_completed(future_to_file):
            file_path, file_hash, error = future.result()
            if error:
                logger.error(f"Failed to hash {file_path}: {error}")
            else:
                relative_path = os.path.relpath(file_path, source_folder)
                source_hashes[relative_path] = file_hash

    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {executor.submit(hash_file, file_path): file_path for file_path in destination_file_paths}
        for future in concurrent.futures.as_completed(future_to_file):
            file_path, file_hash, error = future.result()
            if error:
                logger.error(f"Failed to hash {file_path}: {error}")
            else:
                relative_path = os.path.relpath(file_path, destination_folder)
                destination_hashes[relative_path] = file_hash

    # Compare hashes
    discrepancies = compare_hashes(source_hashes, destination_hashes)

    # Write discrepancies to CSV
    with open(output_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["File Path", "Status", "Source Hash", "Destination Hash"])
        writer.writerows(discrepancies)

    print(f"Hashes have been successfully saved to {output_file}")
    print(f"Discrepancies have been saved to {output_file}")
    monitor_thread.cancel()  # Stop the resource monitoring thread

if __name__ == "__main__":
    main()
